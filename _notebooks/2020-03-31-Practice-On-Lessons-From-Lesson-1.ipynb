{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice using the Lesson 1 notebook\n",
    "> Notes diving deeper into the Lesson 1 notebook from V4 of the Practical Deep Learning for Coders course\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- author: Nissan Dookeran\n",
    "- categories: [fastai, course_v4]\n",
    "- show_tags: true\n",
    "- comments: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes from Jeremy's Lesson 1 lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Lesson 1 of fastbook](https://github.com/fastai/fastbook/blob/master/01_intro.ipynb) as the basis for [Lesson 1 of the V4 course](https://github.com/fastai/course-v4/blob/master/nbs/01_intro.ipynb) currently being run we're using copies of the notebook code embedded in their new book. So below are my experiments from what I learnt here, starting from scratch but drawing heavily from their textbook code. I run this locally on Windows 10 (I can see Jeremy's eye rolling if he reads this since he recommended against this for beginners) but in my defense I did do the course last year and had my laptop configured with V1 of Fastai running so thought I'd see where the stress points were, but to keep productive I also run on Google Collab which is honestly much easier. As a caveat, I'm writing this article while running Jupyter Notebook on the Windows 10 environment so if some code looks slightly modified I'll try to mention explicitly this is a Win10 specific change. Now, on to the experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First if you're running this locally make sure you have the \"Trusted\" button enable for this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to make sure fastai is installed into my local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastai2 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (0.0.13)\n",
      "Requirement already satisfied: requests in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from fastai2) (2.23.0)\n",
      "Requirement already satisfied: pyyaml in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from fastai2) (5.3)\n",
      "Requirement already satisfied: spacy in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from fastai2) (2.1.8)\n",
      "Requirement already satisfied: pillow in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from fastai2) (7.0.0)\n",
      "Requirement already satisfied: fastcore in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from fastai2) (0.1.15)\n",
      "Requirement already satisfied: matplotlib in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from fastai2) (3.1.3)\n",
      "Requirement already satisfied: fastprogress>=0.1.22 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from fastai2) (0.2.2)\n",
      "Requirement already satisfied: scipy in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from fastai2) (1.4.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from fastai2) (1.4.0)\n",
      "Requirement already satisfied: pandas in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from fastai2) (1.0.2)\n",
      "Requirement already satisfied: torchvision>=0.5 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from fastai2) (0.5.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from fastai2) (0.22.2.post1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from requests->fastai2) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from requests->fastai2) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from requests->fastai2) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from requests->fastai2) (2.9)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from spacy->fastai2) (0.1.0)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from spacy->fastai2) (2.0.1)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from spacy->fastai2) (0.2.4)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from spacy->fastai2) (7.0.8)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from spacy->fastai2) (0.9.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from spacy->fastai2) (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from spacy->fastai2) (1.18.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from spacy->fastai2) (0.2.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from spacy->fastai2) (1.0.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from matplotlib->fastai2) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from matplotlib->fastai2) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from matplotlib->fastai2) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from matplotlib->fastai2) (2.4.6)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from pandas->fastai2) (2019.3)\n",
      "Requirement already satisfied: six in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from torchvision>=0.5->fastai2) (1.14.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from scikit-learn->fastai2) (0.14.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from thinc<7.1.0,>=7.0.8->spacy->fastai2) (4.43.0)\n",
      "Requirement already satisfied: setuptools in c:\\tools\\anaconda3\\envs\\fastai2\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->fastai2) (46.0.0.post20200309)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastai2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the notebook and the V4 course repo, Jeremy hides alot of the guts of getting things up and running with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f8223ec046d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;31m#this will crash if you run it outside the folder since `utils.py` is in the root folder of the notebooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils import * #this will crash if you run it outside the folder since `utils.py` is in the root folder of the notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since I like to dig into things I'm going to try to parse the `utils.py` file and pull out just the pieces we need to get Lesson 1 stuff up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also want to test my environment to make sure I have a Nvidia GPU and probe for some details on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x2b2f58b1c48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro P2000'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch found cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.cuda\n",
    "if torch.cuda.is_available():\n",
    "    print('PyTorch found cuda')\n",
    "else:\n",
    "    print('PyTorch could not find cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a block of code in the notebook that executes a learner on images data of both cats and dogs and their breeds. I'm segmenting that code below and putting notes on each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First he gets the data from the built in `URLS` object for `PETS` data which is a simple Amazon S3 storage bucket location containing what looks like either the originally or slightly modified copy of [The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz\n"
     ]
    }
   ],
   "source": [
    "print(URLs.PETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we use a method `untar_data` to decompress this into our local `images` folder. In the next block if you uncomment the code and press `Shift-Tab` 3 (or 4 times for a pop out window) you can see the documentation on the function and the parameters it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function fastai2.data.external.untar_data(url, fname=None, dest=None, c_key='data', force_download=False, extract_func=<function file_extract at 0x000002B2F4592948>)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untar_data #Press `Enter` to see some info or use `Shift+Tab` 3 or 4 times to see more formatted documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)/'images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this had me scratching for awhile when I read the notebook but then in the video Jeremy explained the need for this function. Apparently because it is a mixture of both cats and dogs and their breeds, the way to identify data as being for a cat is to Capitalize The First Letter For Any Cat Breed. So the function below would be reused when classifying data as a Cat before going deeper into identifying its breed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cat(x): return x[0].isupper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ImageDataLoaders` function is handy as it has a variety of extensions to make loading from various image sources much easier. FastAI has this concept of a `DataLoader` that holds the data we're going to train and validate our model on. Because I'm running this on Windows 10, I had to add an additional parameter of `num_workers=0` to the call to make sure it doesn't crash. For Linux users, you can safely remove this call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'ImageDataLoaders' has no attribute 'from_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4e80285fe0f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mImageDataLoaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_\u001b[0m \u001b[1;31m#press Tab once to see all possible calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'ImageDataLoaders' has no attribute 'from_'"
     ]
    }
   ],
   "source": [
    "ImageDataLoaders.from_ #press Tab once to see all possible calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=Resize(224), num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we're going to call the learner that will train the model to recognise the various cats and dogs breeds in our model using the [ResNet34](https://arxiv.org/abs/1512.03385) pre-trained model. A pre-trained model is a very cool concept. Normally to start training a series of weights are chosen at random and over time are changed to better match a specific set of data. These weights are re-usable since basically it lets us jumpstart training a new, related dataset as these pre-trained series of weights are a better starting point than \"random\" for the new more niche subset of related data. Here we also specifiy the metric we're using as a measure of accuracy, fastai defines one for us that's useful, appropriately called `error_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function fastai2.metrics.error_rate(inp, targ, axis=-1)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rate # Press Shift+Tab 3 times to see the documentation on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet34, metrics=error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast.ai V2 also introduced this nice method called `fine_tune` which basially runs a cycle of training again using a learning rate it has pre-determined as best for training to \"fine tune\" the model to produce better accuracy. I won't propose to understand it fully yet, but from what I know it freezes a fixed number of the hidden layers of the pre-trained model, and unfreezes the last few layers so their weights can be changed to better match the niche dataset. The logic is sound as the first set of layers would already be really good at recognising general features common in all images of the superset, but the last few layers can now be fine tuned to match the explicit features in the specific classifications of the niche dataset we are presenting for training in this model. As I learn I may write more about how I understand this function to work and append this post to match what I learn. Note, this step might take some time to execute depending on the speed and memory of the GPU you have running. Google Collab would take about 5 minutes. My local laptop running Windows 10 takes a little longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.167801</td>\n",
       "      <td>0.024875</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>07:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.052814</td>\n",
       "      <td>0.014244</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>05:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fine_tune(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a trained model that looks like it has a pretty low error rate and we want to test it with something it hasn't seen before. This library in the next code block was added early in V3 of the course and is more prominent in V4 and lets you put GUI elements you're used to in a normal application (like a file uploader) into the notebook. I did a quick web search for a cat, downloaded that file and used it here to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601374617753497db5805fceb0790573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "uploader = widgets.FileUpload()\n",
    "uploader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code simply looks at the image we loaded, passes it to our model for an evaluation of whether or not it thinks it's a cat. I'd recommend having both a dog and a cat image available to test on your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this a cat?: True.\n",
      "Probability it's a cat: 1.000000\n"
     ]
    }
   ],
   "source": [
    "img = PILImage.create(uploader.data[0])\n",
    "is_cat,_,probs = learn.predict(img)\n",
    "print(f\"Is this a cat?: {is_cat}.\")\n",
    "print(f\"Probability it's a cat: {probs[1].item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you go. That simple. You're now a deep learning practitioner like me so celebrate the accomplishment. \n",
    "Hopefully you feel less like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> youtube: https://youtu.be/ygFoN3hKi20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And more like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> youtube: https://youtu.be/535Zy_rf4NU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes from first few minutes Jeremy's Lesson 2 lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeremy continued to cover the Chapter 1 notebook from Fastbook in Lesson 2. Appending the notes here to keep them naturally aligned to the Fastbook content for when I get my copy (or you yours) to be able to map it all back more easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First bring back the code we're taking a look at\n",
    "\n",
    "> Tip: \n",
    "- Use the [fastbook book git repository](https://github.com/fastai/fastbook) as well as the [course-v4 repository](https://github.com/fastai/course-v4) for reference. The *fastbook* repo should be used as reading material while the *course-v4* repo can be used to just see the sample code and experiment to ensure you're understanding the material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.all import * #since I don't have utils.py here, will just pull the lines from it we need\n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "def is_cat(x): return x[0].isupper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the bit he's exploring for the notes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=Resize(224), num_workers=0) #num_workers=0 is needed for Windows only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First thing to note, is we have a labelling function `label_func`, in this case `is_cat(x)` which all it does is check the first letter of the file name, because the files are labelled in such a way that an UPPERCASE first letter in the file name indicates it is a cat breed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This example code allows us to predict a category, i.e. the label is a category. In this case, our model is a *classification model*\n",
    "    > If we're trying to predict a variable number, like an age, or location then this is a *regression model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What `valid_pct=0.2` does in the sample code given is puts aside 20% of the data in training to test the model to see how accurate the model is. This is called a *validation set* and is useful in detecting *overfitting*\n",
    "- Overfitting happens when you train the model for too long, or with too little data or with too many parameters, the accuracy of the model starts to go down, since it measures predictions on the validation set and when this value starts to go down, this is a sign of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking at `cnn_learner(dls, resnet34, metrics=error_rate)`. This is a *learner*, which contains your data (`dls`), a function (`resnet34`) which we are optimising for our data, and a list of functions (`metrics=error_rate`) you want executed and printed out after running each *epoch*. It figures out what are the *parameters* that best cause the function `resnet34` to best match the labels in our data in `dls`.\n",
    "    - learner\n",
    "    - parameters\n",
    "    - epoch - when you look at every image in the training set once.\n",
    "    - `resnet34` is a particular *architecture* [Resnet](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035) that has been shown to be very good at computer vision problems. 34 in `resnet34` indicates how many layers there are. The higher numbered versions (resnet50 etc) take more parameters, use more memory, take longer to train, are more likely to overfit but can also create more complex models\n",
    "    - architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`metrics` is a function that measures the quality of the predictions using our *validation set*.\n",
    "`error_rate` prints out what percent of the training dataset are being incorrectly classified by the model. `accuracy` is another metric and is calculated as (1 - *error_rate*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is potentially a difference between the *metric* and the *loss*, but they are closely related. Attribution to [Arthur Samuel](https://en.wikipedia.org/wiki/Arthur_Samuel)'s insight here. \n",
    "- We need a way to measure how well our model is doing so when we change the *parameters* we cna figure out which set of parameters make that performance measurement get better or worse. The *loss* is that performance measurement.\n",
    "\n",
    "*Error rate* and *accuracy* do not indicate if we are getting better or worse if we change our *parameters* just a little bit, since we may overall be predicting exactly the same number of items correctly, so these are not measures of *loss* since potentially we could be not predicting as well on the data in the validation set once the parameters are changed, but still well enough to give the labelled output value as the prediction.\n",
    "You generally care most about the metrics, but the program cares about the loss, since this tells it how to update the parameters when running the model against the training data in the next *epoch*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We care about how well the model predicts not just with the data we're training with, but with data it has never seen before.\n",
    "- While we can *cheat* by developing a model that generates great metrics for our validation set, we can set aside a 3rd set of data, a *test set* that isn't used in either training or generating validation metrics for the model, but only used when the entire project is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeremy starts going into fine-tuning, transfer learning, and how using AlexNet example what makes sense intuitively is proven.\n",
    "Parameters vs hyper-parameters. TODO is add notes from here as I go over again this material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
